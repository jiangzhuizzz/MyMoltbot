# 客户搜索功能实现详解

> 目标：清楚理解"客户搜索"从需求到实现的完整流程
> 
> 最后更新: 2026-02-01

---

## 🎯 一、功能概述

### 1.1 什么是客户搜索？

**客户搜索** = 在互联网上找到有贷款需求的潜在客户

```
输入：关键词（如"贷款"、"征信逾期"）
  │
  ▼
处理：多平台搜索 → 数据采集 → 意向分析 → 线索生成
  │
  ▼
输出：客户线索列表（带意向评分）
```

### 1.2 完整流程图

```
┌─────────────────────────────────────────────────────────────────┐
│                      客户搜索完整流程                            │
│                                                                 │
│  ┌──────────┐                                                  │
│  │ 1.输入   │ ← 你提供关键词                                   │
│  │ 关键词   │   "贷款"、"征信逾期"、"急需资金"                 │
│  └────┬─────┘                                                  │
│       │                                                         │
│       ▼                                                         │
│  ┌──────────┐                                                  │
│  │ 2.多平台 │                                                  │
│  │ 搜索     │ → 百度/知乎/抖音/小红书/房产平台                  │
│  └────┬─────┘                                                  │
│       │                                                         │
│       ▼                                                         │
│  ┌──────────┐                                                  │
│  │ 3.数据   │                                                  │
│  │ 采集     │ → 爬取搜索结果                                   │
│  └────┬─────┘                                                  │
│       │                                                         │
│       ▼                                                         │
│  ┌──────────┐                                                  │
│  │ 4.意向   │                                                  │
│  │ 分析     │ → AI判断需求强度（高/中/低）                      │
│  └────┬─────┘                                                  │
│       │                                                         │
│       ▼                                                         │
│  ┌──────────┐                                                  │
│  │ 5.线索   │                                                  │
│  │ 生成     │ → 存入数据库                                     │
│  └────┬─────┘                                                  │
│       │                                                         │
│       ▼                                                         │
│  ┌──────────┐                                                  │
│  │ 6.同步   │                                                  │
│  │ 飞书     │ → 自动导入CRM                                    │
│  └──────────┘                                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 🔍 二、数据来源（从哪搜）

### 2.1 搜索平台分类

| 类别 | 平台 | 数据类型 | 客户质量 | 难度 |
|------|------|----------|----------|------|
| **搜索引擎** | 百度、知乎 | 问答、帖子 | ⭐⭐⭐ | 低 |
| **社交媒体** | 抖音、小红书 | 视频评论、笔记 | ⭐⭐⭐⭐ | 中 |
| **房产平台** | 贝壳、链家 | 房源咨询 | ⭐⭐⭐⭐ | 中 |
| **社区论坛** | 贴吧、虎扑 | 帖子、评论 | ⭐⭐ | 低 |
| **本地生活** | 美团、大众点评 | 商家信息 | ⭐⭐⭐ | 中 |

### 2.2 具体数据源

#### 搜索引擎
```
┌────────────────────────────────────────────────────────────┐
│  百度搜索                                                    │
│  ├── 百度知道：用户提问"贷款被拒怎么办"                      │
│  ├── 百度贴吧：帖子"征信逾期能贷款吗"                        │
│  └── 百度经验：文章"如何申请信用贷"                          │
├────────────────────────────────────────────────────────────┤
│  知乎                                                        │
│  ├── 问答："征信不好怎么贷款"                                │
│  ├── 文章：贷款攻略                                          │
│  └── 专栏：金融知识                                          │
└────────────────────────────────────────────────────────────┘
```

#### 社交媒体
```
┌────────────────────────────────────────────────────────────┐
│  抖音                                                        │
│  ├── 视频评论区："急需贷款但征信不好"                        │
│  ├── 搜索结果：贷款相关视频                                  │
│  └── 话题：#贷款 #征信 #急用钱                               │
├────────────────────────────────────────────────────────────┤
│  小红书                                                      │
│  ├── 笔记："征信不好怎么贷款"                                │
│  ├── 评论："哪个银行利息低"                                  │
│  └── 话题：#贷款 #征信逾期 #装修贷款                         │
└────────────────────────────────────────────────────────────┘
```

#### 房产平台
```
┌────────────────────────────────────────────────────────────┐
│  贝壳找房                                                    │
│  ├── 房源咨询："这套房能贷款吗"                              │
│  ├── 经纪人问答：贷款问题                                    │
│  └── 小区论坛：业主资金需求                                  │
├────────────────────────────────────────────────────────────┤
│  链家                                                        │
│  ├── 问答区：贷款咨询                                        │
│  └── 房源讨论：首付不够怎么办                                │
└────────────────────────────────────────────────────────────┘
```

---

## 🛠️ 三、实现方式

### 3.1 实现方案对比

| 方案 | 费用 | 难度 | 数据量 | 稳定性 | 推荐度 |
|------|------|------|--------|--------|--------|
| **网页爬虫** | 免费 | 中 | 中 | ⚠️ 不稳定 | ⭐⭐⭐⭐ |
| **官方API** | 免费/付费 | 中 | 大 | ✅ 稳定 | ⭐⭐⭐⭐⭐ |
| **第三方数据** | 付费 | 低 | 大 | ✅ 稳定 | ⭐⭐⭐ |

### 3.2 推荐方案：网页爬虫（当前）

**原理**：模拟浏览器访问网页，抓取公开数据

```
请求 → 服务器 → 返回HTML → 解析 → 提取数据 → 存储
```

**优点**：
- 免费
- 不需要官方审批
- 灵活性高

**缺点**：
- 可能被封（需要代理、延时）
- 数据不完整

### 3.3 代码实现结构

```
customer-search/
├── crawler/
│   ├── base.py          # 爬虫基类
│   ├── baidu.py         # 百度爬虫
│   ├── zhihu.py         # 知乎爬虫
│   ├── douyin.py        # 抖音爬虫
│   └── xiaohongshu.py   # 小红书爬虫
├── analyzer/
│   ├── intent.py        # 意向分析
│   └── keyword.py       # 关键词处理
├── storage/
│   └── lead.py          # 线索存储
├── config.py            # 配置文件
└── main.py              # 主程序
```

---

## 📊 四、核心代码实现

### 4.1 爬虫基类

```python
import requests
from abc import ABC, abstractmethod
from typing import List, Dict
import time
import random

class BaseCrawler(ABC):
    """爬虫基类"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    @abstractmethod
    def search(self, keyword: str) -> List[Dict]:
        """搜索方法，子类实现"""
        pass
    
    def random_delay(self, min_sec: int = 2, max_sec: int = 5):
        """随机延时，避免被封"""
        time.sleep(random.randint(min_sec, max_sec))
    
    def safe_request(self, url: str, retries: int = 3) -> str:
        """安全请求，带重试"""
        for i in range(retries):
            try:
                resp = self.session.get(url, timeout=10)
                if resp.status_code == 200:
                    return resp.text
            except Exception as e:
                if i == retries - 1:
                    return ""
                time.sleep(2)
        return ""
```

### 4.2 百度搜索爬虫

```python
import re
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

class BaiduCrawler(BaseCrawler):
    """百度搜索爬虫"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://www.baidu.com/s"
    
    def search(self, keyword: str) -> List[Dict]:
        """搜索百度"""
        results = []
        
        # 构造搜索URL
        params = {
            'wd': keyword,
            'pn': 0,  # 页码
        }
        
        url = f"{self.base_url}?{quote_plus(keyword)}"
        
        # 发送请求
        html = self.safe_request(url)
        if not html:
            return results
        
        # 解析HTML
        soup = BeautifulSoup(html, 'html.parser')
        
        # 提取搜索结果
        for item in soup.select('.c-container')[:10]:
            try:
                title = item.select_one('.t')
                if not title:
                    continue
                
                title = title.get_text(strip=True)
                link = item.select_one('a')['href']
                desc = item.select_one('.c-abstract')
                desc = desc.get_text(strip=True) if desc else ""
                
                # 提取作者信息（如果有）
                author = ""
                author_elem = item.select_one('.c-author')
                if author_elem:
                    author = author_elem.get_text(strip=True)
                
                results.append({
                    'platform': '百度',
                    'type': '搜索结果',
                    'title': title,
                    'content': desc,
                    'author': author,
                    'url': link,
                    'keyword': keyword,
                    'raw_html': str(item)
                })
            except Exception as e:
                continue
        
        self.random_delay()
        return results
```

### 4.3 知乎爬虫

```python
class ZhihuCrawler(BaseCrawler):
    """知乎爬虫"""
    
    def __init__(self):
        super().__init__()
        self.search_url = "https://www.zhihu.com/search"
    
    def search(self, keyword: str) -> List[Dict]:
        """搜索知乎"""
        results = []
        
        params = {
            'type': 'content',
            'q': keyword
        }
        
        url = f"{self.search_url}?{quote_plus(keyword)}"
        html = self.safe_request(url)
        
        if not html:
            return results
        
        soup = BeautifulSoup(html, 'html.parser')
        
        for item in soup.select('.SearchResult-Card')[:10]:
            try:
                title_elem = item.select_one('h2 a')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = 'https://www.zhihu.com' + title_elem['href']
                
                desc_elem = item.select_one('.SearchResult-description')
                desc = desc_elem.get_text(strip=True) if desc_elem else ""
                
                author_elem = item.select_one('.SearchItem-author')
                author = author_elem.get_text(strip=True) if author_elem else "匿名用户"
                
                results.append({
                    'platform': '知乎',
                    'type': '问答/文章',
                    'title': title,
                    'content': desc,
                    'author': author,
                    'url': link,
                    'keyword': keyword
                })
            except Exception as e:
                continue
        
        self.random_delay(3, 8)
        return results
```

---

## 🧠 五、意向分析（AI判断）

### 5.1 关键词匹配法

```python
class IntentAnalyzer:
    """意向分析器"""
    
    def __init__(self):
        # 高意向关键词
        self.high_intent_keywords = [
            '急需贷款', '征信逾期', '贷款被拒', '征信不好',
            '黑户贷款', '无条件贷款', '当天放款', '贷款下不来',
            '急需资金', '急用钱', '贷款审批'
        ]
        
        # 中意向关键词
        self.medium_intent_keywords = [
            '贷款利息', '哪个银行', '贷款条件', '怎么贷款',
            '贷款利率', '能贷多少', '首次贷款', '信用贷款',
            '贷款流程', '贷款要求', '商业贷款'
        ]
        
        # 低意向关键词
        self.low_intent_keywords = [
            '贷款', '借钱', '资金', '周转', '买房',
            '装修', '买车', '创业', '投资', '分期'
        ]
        
        # 排除词（不是目标客户）
        self.exclude_keywords = [
            '诈骗', '骗子', '黑中介', '骗局', '套路贷',
            '不要相信', '警惕', '虚假', '违法', '投诉'
        ]
    
    def analyze(self, content: str, title: str = "") -> Dict:
        """分析意向"""
        text = (title + ' ' + content).lower()
        score = 0
        matched_keywords = []
        
        # 匹配高意向
        for kw in self.high_intent_keywords:
            if kw.lower() in text:
                score += 30
                matched_keywords.append(kw)
        
        # 匹配中意向
        for kw in self.medium_intent_keywords:
            if kw.lower() in text:
                score += 15
                matched_keywords.append(kw)
        
        # 匹配低意向
        for kw in self.low_intent_keywords:
            if kw.lower() in text:
                score += 5
                matched_keywords.append(kw)
        
        # 排除词扣分
        for kw in self.exclude_keywords:
            if kw.lower() in text:
                score = max(0, score - 50)
                matched_keywords.append(f'[排除]{kw}')
        
        # 计算关键词匹配度奖励
        bonus = min(len(set(matched_keywords)) * 3, 30)
        score = min(score + bonus, 100)
        
        # 意向等级
        if score >= 70:
            level = '高意向'
        elif score >= 40:
            level = '中意向'
        elif score >= 10:
            level = '低意向'
        else:
            level = '无意向'
        
        return {
            'score': score,
            'level': level,
            'matched_keywords': matched_keywords
        }
```

### 5.2 使用示例

```python
analyzer = IntentAnalyzer()

# 测试
test_cases = [
    ("征信有逾期还能贷款吗？急！", "征信逾期能贷款吗"),
    ("哪个银行贷款利息低一点", "贷款利息"),
    ("想买个房子了解一下贷款", "买房贷款"),
    ("警惕贷款诈骗，大家不要上当", "贷款诈骗")
]

for content, title in test_cases:
    result = analyzer.analyze(content, title)
    print(f"内容: {content[:30]}...")
    print(f"  意向: {result['level']} ({result['score']}分)")
    print(f"  关键词: {result['matched_keywords']}")
```

**输出**：
```
内容: 征信有逾期还能贷款吗？急！...
  意向: 高意向 (85分)
  关键词: ['征信逾期', '急需贷款']

内容: 哪个银行贷款利息低一点...
  意向: 中意向 (45分)
  关键词: ['贷款利息']

内容: 想买个房子了解一下贷款...
  意向: 低意向 (15分)
  关键词: ['贷款', '买房']

内容: 警惕贷款诈骗，大家不要上当...
  意向: 无意向 (0分)
  关键词: ['[排除]诈骗', '[排除]警惕']
```

---

## 💾 六、数据存储

### 6.1 线索数据结构

```python
@dataclass
class Lead:
    """客户线索"""
    id: str                          # 唯一ID
    platform: str                    # 来源平台
    type: str                        # 内容类型（问答/视频/笔记）
    keyword: str                     # 触发关键词
    title: str                       # 标题
    content: str                     # 内容摘要
    author: str                      # 作者
    url: str                         # 原文链接
    intent_score: int                # 意向评分 (0-100)
    intent_level: str                # 意向等级（高/中/低）
    status: str                      # 状态（新线索/已联系/已导入/无效）
    created_at: str                  # 创建时间
    synced_at: str = ""              # 同步到CRM时间
    followed_at: str = ""            # 跟进时间
```

### 6.2 存储示例（JSON）

```json
{
  "id": "L001",
  "platform": "知乎",
  "type": "问答",
  "keyword": "贷款",
  "title": "征信不好怎么贷款？急！",
  "content": "之前信用卡逾期过几次，现在急用钱怎么办",
  "author": "李先生",
  "url": "https://www.zhihu.com/question/123",
  "intent_score": 85,
  "intent_level": "高意向",
  "status": "新线索",
  "created_at": "2026-02-01 10:00",
  "synced_at": "",
  "followed_at": ""
}
```

---

## 🔄 七、完整调用流程

```python
class CustomerSearcher:
    """客户搜索器"""
    
    def __init__(self):
        self.crawlers = {
            '百度': BaiduCrawler(),
            '知乎': ZhihuCrawler(),
            '抖音': DouyinCrawler(),  # 需实现
            '小红书': XiaohongshuCrawler(),  # 需实现
        }
        self.analyzer = IntentAnalyzer()
        self.leads = []
    
    def search(self, keyword: str) -> List[Lead]:
        """搜索客户"""
        print(f"\n🔍 开始搜索: {keyword}")
        
        all_results = []
        
        # 1. 多平台搜索
        for platform, crawler in self.crawlers.items():
            print(f"  📡 搜索 {platform}...")
            try:
                results = crawler.search(keyword)
                all_results.extend(results)
                print(f"     找到 {len(results)} 条")
            except Exception as e:
                print(f"     搜索失败: {e}")
        
        # 2. 意向分析
        print(f"\n🧠 分析意向...")
        leads = []
        for item in all_results:
            content = item.get('content', '') + item.get('title', '')
            analysis = self.analyzer.analyze(content)
            
            lead = Lead(
                id=self.generate_id(),
                platform=item['platform'],
                type=item.get('type', '搜索结果'),
                keyword=keyword,
                title=item.get('title', ''),
                content=content[:200],
                author=item.get('author', '匿名'),
                url=item.get('url', ''),
                intent_score=analysis['score'],
                intent_level=analysis['level'],
                status='新线索',
                created_at=datetime.now().strftime('%Y-%m-%d %H:%M')
            )
            leads.append(lead)
        
        # 3. 按意向排序
        leads.sort(key=lambda x: x.intent_score, reverse=True)
        
        # 4. 统计
        high = len([l for l in leads if l.intent_level == '高意向'])
        medium = len([l for l in leads if l.intent_level == '中意向'])
        
        print(f"\n📊 搜索结果:")
        print(f"   总数: {len(leads)}")
        print(f"   高意向: {high}")
        print(f"   中意向: {medium}")
        
        self.leads = leads
        return leads
    
    def export(self) -> str:
        """导出为JSON"""
        data = [asdict(l) for l in self.leads]
        filename = f"leads_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return filename


# 使用示例
if __name__ == '__main__':
    searcher = CustomerSearcher()
    
    # 搜索
    leads = searcher.search("贷款")
    
    # 导出
    file = searcher.export()
    print(f"\n💾 已导出: {file}")
```

---

## 📊 八、执行效果

### 8.1 搜索日志示例

```
🔍 开始搜索: 贷款

  📡 搜索 百度...
     找到 10 条
  📡 搜索 知乎...
     找到 8 条
  📡 搜索 抖音...
     找到 5 条
  📡 搜索 小红书...
     找到 7 条

🧠 分析意向...

📊 搜索结果:
   总数: 30
   高意向: 8
   中意向: 15

💾 已导出: leads_20260201_100000.json
```

### 8.2 输出示例

| 平台 | 类型 | 客户 | 意向 | 内容摘要 |
|------|------|------|------|----------|
| 知乎 | 问答 | 李先生 | 高意向 (85) | 征信有逾期能贷款吗？急！ |
| 小红书 | 笔记 | 张女士 | 高意向 (80) | 征信花了能贷多少 |
| 抖音 | 评论 | 王同学 | 中意向 (55) | 哪个银行贷款利息低 |
| 百度 | 搜索 | 赵女士 | 低意向 (25) | 想了解一下贷款流程 |

---

## 🎯 九、下一步优化

### 短期（1周）
- [ ] 实现更多爬虫（抖音、小红书）
- [ ] 优化意向分析算法
- [ ] 添加代理池防封

### 中期（1个月）
- [ ] 接入官方API（百度、知乎）
- [ ] 添加房产平台爬虫
- [ ] 实现定时自动搜索

### 长期（3个月）
- [ ] AI大模型分析（更精准）
- [ ] 图像识别（OCR）
- [ ] 语音分析

---

## 💡 一句话总结

**客户搜索** = 多平台爬取数据 → AI分析意向 → 生成可跟进线索

| 步骤 | 工具 | 输出 |
|------|------|------|
| 1. 爬取数据 | Python爬虫 | 原始数据 |
| 2. 分析意向 | 关键词匹配 | 意向评分 |
| 3. 存储数据 | JSON文件 | 线索列表 |
| 4. 同步CRM | 飞书API | 客户档案 |

---

*文档由 AI 自动生成*
